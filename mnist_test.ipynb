{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script reproduces what the main.py function does but divided into its parts so we can visualize the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Data_handling\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from utils.datasets_generation import generate_dataset\n",
    "import utils.losses as losses\n",
    "from utils.train_test_loop import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'mnist'\n",
    "corruption = 'Noisy_Patrini_MNIST'\n",
    "corr_p = 0.7\n",
    "corr_n = None\n",
    "loss_type = 'Forward'\n",
    "\n",
    "for i in range(reps):\n",
    "        generate_dataset(dataset=dataset,corruption=corruption,corr_p=corr_p,repetitions=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "Epoch 1/10: Train Loss: 2.6349, Train Acc: 0.7526, Test Acc: 0.7704, Train Detached Loss: 1.4412, Test Detached Loss: 1.4818, Learning Rate: 0.001000\n",
      "Epoch 2/10: Train Loss: 2.2664, Train Acc: 0.7667, Test Acc: 0.7658, Train Detached Loss: 3.0852, Test Detached Loss: 3.1539, Learning Rate: 0.001000\n",
      "Epoch 3/10: Train Loss: 2.2729, Train Acc: 0.7676, Test Acc: 0.7772, Train Detached Loss: 3.7580, Test Detached Loss: 3.8099, Learning Rate: 0.001000\n",
      "Epoch 4/10: Train Loss: 2.0868, Train Acc: 0.7756, Test Acc: 0.7714, Train Detached Loss: 4.2651, Test Detached Loss: 4.3548, Learning Rate: 0.001000\n",
      "Epoch 5/10: Train Loss: 2.1131, Train Acc: 0.7739, Test Acc: 0.7819, Train Detached Loss: 4.1922, Test Detached Loss: 4.1603, Learning Rate: 0.001000\n",
      "Epoch 6/10: Train Loss: 2.0643, Train Acc: 0.7791, Test Acc: 0.7902, Train Detached Loss: 4.2147, Test Detached Loss: 4.2658, Learning Rate: 0.001000\n",
      "Epoch 7/10: Train Loss: 2.0360, Train Acc: 0.7798, Test Acc: 0.7819, Train Detached Loss: 4.6557, Test Detached Loss: 4.7318, Learning Rate: 0.001000\n",
      "Epoch 8/10: Train Loss: 2.0189, Train Acc: 0.7820, Test Acc: 0.7862, Train Detached Loss: 5.2350, Test Detached Loss: 5.3255, Learning Rate: 0.001000\n",
      "Epoch 9/10: Train Loss: 1.9922, Train Acc: 0.7934, Test Acc: 0.7998, Train Detached Loss: 4.4997, Test Detached Loss: 4.5292, Learning Rate: 0.001000\n",
      "Epoch 10/10: Train Loss: 2.0359, Train Acc: 0.7869, Test Acc: 0.7822, Train Detached Loss: 5.7661, Test Detached Loss: 5.9117, Learning Rate: 0.001000\n",
      "784\n",
      "Epoch 1/10: Train Loss: 2.7899, Train Acc: 0.7757, Test Acc: 0.8474, Train Detached Loss: 0.7114, Test Detached Loss: 0.7058, Learning Rate: 0.001000\n",
      "Epoch 2/10: Train Loss: 2.0169, Train Acc: 0.8613, Test Acc: 0.8829, Train Detached Loss: 0.7641, Test Detached Loss: 0.7339, Learning Rate: 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m     mlp \u001b[38;5;241m=\u001b[39m MLP(Data\u001b[38;5;241m.\u001b[39mnum_features, [], Weak\u001b[38;5;241m.\u001b[39mc, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, bn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m     optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(mlp\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     mlp,results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcorr_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorr_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Bwd = FwdBwdLoss(pinv(M),I_c)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Fwd = FwdBwdLoss(I_d,M)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\utils\\train_test_loop.py:33\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, trainloader, testloader, optimizer, loss_fn, num_epochs, corr_p, rep, sound, loss_type)\u001b[0m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\utils\\losses.py:80\u001b[0m, in \u001b[0;36mFwdBwdLoss.forward\u001b[1;34m(self, inputs, z)\u001b[0m\n\u001b[0;32m     78\u001b[0m log_Ff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(Ff\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m     79\u001b[0m B_log_Ff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m log_Ff\n\u001b[1;32m---> 80\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(B_log_Ff[z,\u001b[38;5;28mrange\u001b[39m(B_log_Ff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#L = - torch.sum(B_log_Ff[z,range(B_log_Ff.size(1))]+1e-10)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\.venv\\lib\\site-packages\\torch\\_tensor.py:41\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\Desktop\\VS projects\\ForwardBackard_losses\\.venv\\lib\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    376\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 379\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(reps):\n",
    "    base_dir = \"Datasets/weak_datasets\"\n",
    "    if corr_n is not None:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "    else:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "    f = open(folder_path + f'/Dataset_{i}.pkl','rb')\n",
    "    Data,Weak = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    if loss_type == 'Backward':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y,np.eye(Weak.c))\n",
    "    elif loss_type == 'Forward':\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'Forward_opt':\n",
    "        B = Weak.M @ torch.inverse(self.M.T @ torch.inverse(torch.diag(self.pest)) @ self.M ) @ self.M.T @ torch.inverse(torch.diag(self.pest))\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'EM':\n",
    "        loss_fn = losses.EMLoss(Weak.M)\n",
    "    elif loss_type == 'LBL':\n",
    "        loss_fn = losses.LBLoss()\n",
    "    elif loss_type == 'Backward_opt':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_opt_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'OSL':\n",
    "        loss_fn = losses.OSLCELoss()\n",
    "    \n",
    "    if loss_type == 'OSL':\n",
    "        Data.include_weak(Weak.w)\n",
    "    else:\n",
    "        Data.include_weak(Weak.z)\n",
    "\n",
    "    \n",
    "    trainloader,testloader = Data.get_dataloader(weak_labels = 'weak')\n",
    "\n",
    "    print(Data.num_features)\n",
    "\n",
    "\n",
    "    #trainloader,testloader = Data.get_dataloader()\n",
    "    mlp = MLP(Data.num_features, [], Weak.c, dropout_p=0, bn = False, activation='id')\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=1e-3,betas=(0.9, 0.999), eps=1e-8)\n",
    "    mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss_fn,corr_p=corr_p,num_epochs=10,sound=1,rep=i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bwd = FwdBwdLoss(pinv(M),I_c)\n",
    "# Fwd = FwdBwdLoss(I_d,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_detached_loss</th>\n",
       "      <th>test_detached_loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss_fn</th>\n",
       "      <th>repetition</th>\n",
       "      <th>initial_lr</th>\n",
       "      <th>actual_lr</th>\n",
       "      <th>corr_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.634868</td>\n",
       "      <td>0.752617</td>\n",
       "      <td>0.7704</td>\n",
       "      <td>1.441179</td>\n",
       "      <td>1.481786</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.266381</td>\n",
       "      <td>0.766717</td>\n",
       "      <td>0.7658</td>\n",
       "      <td>3.085225</td>\n",
       "      <td>3.153918</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.272940</td>\n",
       "      <td>0.767583</td>\n",
       "      <td>0.7772</td>\n",
       "      <td>3.757974</td>\n",
       "      <td>3.809920</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.086789</td>\n",
       "      <td>0.775650</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>4.265095</td>\n",
       "      <td>4.354797</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.113070</td>\n",
       "      <td>0.773867</td>\n",
       "      <td>0.7819</td>\n",
       "      <td>4.192219</td>\n",
       "      <td>4.160255</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.064319</td>\n",
       "      <td>0.779083</td>\n",
       "      <td>0.7902</td>\n",
       "      <td>4.214673</td>\n",
       "      <td>4.265758</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2.035951</td>\n",
       "      <td>0.779767</td>\n",
       "      <td>0.7819</td>\n",
       "      <td>4.655750</td>\n",
       "      <td>4.731796</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2.018933</td>\n",
       "      <td>0.781983</td>\n",
       "      <td>0.7862</td>\n",
       "      <td>5.235042</td>\n",
       "      <td>5.325461</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.992198</td>\n",
       "      <td>0.793433</td>\n",
       "      <td>0.7998</td>\n",
       "      <td>4.499726</td>\n",
       "      <td>4.529219</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2.035942</td>\n",
       "      <td>0.786933</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>5.766142</td>\n",
       "      <td>5.911747</td>\n",
       "      <td>Adam</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
       "0      1    2.634868   0.752617    0.7704             1.441179   \n",
       "1      2    2.266381   0.766717    0.7658             3.085225   \n",
       "2      3    2.272940   0.767583    0.7772             3.757974   \n",
       "3      4    2.086789   0.775650    0.7714             4.265095   \n",
       "4      5    2.113070   0.773867    0.7819             4.192219   \n",
       "5      6    2.064319   0.779083    0.7902             4.214673   \n",
       "6      7    2.035951   0.779767    0.7819             4.655750   \n",
       "7      8    2.018933   0.781983    0.7862             5.235042   \n",
       "8      9    1.992198   0.793433    0.7998             4.499726   \n",
       "9     10    2.035942   0.786933    0.7822             5.766142   \n",
       "\n",
       "   test_detached_loss optimizer loss_fn  repetition  initial_lr  actual_lr  \\\n",
       "0            1.481786      Adam    None           0       0.001      0.001   \n",
       "1            3.153918      Adam    None           0       0.001      0.001   \n",
       "2            3.809920      Adam    None           0       0.001      0.001   \n",
       "3            4.354797      Adam    None           0       0.001      0.001   \n",
       "4            4.160255      Adam    None           0       0.001      0.001   \n",
       "5            4.265758      Adam    None           0       0.001      0.001   \n",
       "6            4.731796      Adam    None           0       0.001      0.001   \n",
       "7            5.325461      Adam    None           0       0.001      0.001   \n",
       "8            4.529219      Adam    None           0       0.001      0.001   \n",
       "9            5.911747      Adam    None           0       0.001      0.001   \n",
       "\n",
       "   corr_p  \n",
       "0     0.7  \n",
       "1     0.7  \n",
       "2     0.7  \n",
       "3     0.7  \n",
       "4     0.7  \n",
       "5     0.7  \n",
       "6     0.7  \n",
       "7     0.7  \n",
       "8     0.7  \n",
       "9     0.7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000002"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Weak.M - B.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pest = torch.from_numpy(Weak.generate_wl_priors())\n",
    "tm = torch.from_numpy(Weak.M)\n",
    "B = tm @ torch.inverse(tm.T @ torch.inverse(torch.diag(pest)) @ tm) @ tm.T @ torch.inverse(torch.diag(pest))\n",
    "B.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.Y-Weak.Y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "Epoch 1/10: Train Loss: 2.4305, Train Acc: 0.8500, Test Acc: 0.8799, Train Detached Loss: 0.4478, Test Detached Loss: 0.4365, Learning Rate: 0.001000\n",
      "Epoch 2/10: Train Loss: 2.1110, Train Acc: 0.8773, Test Acc: 0.8975, Train Detached Loss: 0.5769, Test Detached Loss: 0.5284, Learning Rate: 0.001000\n",
      "Epoch 3/10: Train Loss: 2.0091, Train Acc: 0.8852, Test Acc: 0.8954, Train Detached Loss: 0.7091, Test Detached Loss: 0.6975, Learning Rate: 0.001000\n",
      "Epoch 4/10: Train Loss: 1.9082, Train Acc: 0.8916, Test Acc: 0.9044, Train Detached Loss: 0.7677, Test Detached Loss: 0.7466, Learning Rate: 0.001000\n",
      "Epoch 5/10: Train Loss: 1.8812, Train Acc: 0.8938, Test Acc: 0.8974, Train Detached Loss: 1.0000, Test Detached Loss: 0.9904, Learning Rate: 0.001000\n",
      "Epoch 6/10: Train Loss: 1.8379, Train Acc: 0.8967, Test Acc: 0.9083, Train Detached Loss: 0.9244, Test Detached Loss: 0.9052, Learning Rate: 0.001000\n",
      "Epoch 7/10: Train Loss: 1.8364, Train Acc: 0.8965, Test Acc: 0.9135, Train Detached Loss: 0.9985, Test Detached Loss: 0.9771, Learning Rate: 0.001000\n",
      "Epoch 8/10: Train Loss: 1.8091, Train Acc: 0.8988, Test Acc: 0.8919, Train Detached Loss: 1.1642, Test Detached Loss: 1.1901, Learning Rate: 0.001000\n",
      "Epoch 9/10: Train Loss: 1.8076, Train Acc: 0.8990, Test Acc: 0.9081, Train Detached Loss: 1.1457, Test Detached Loss: 1.1131, Learning Rate: 0.001000\n",
      "Epoch 10/10: Train Loss: 1.7423, Train Acc: 0.9024, Test Acc: 0.9024, Train Detached Loss: 1.2240, Test Detached Loss: 1.2287, Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "reps = 10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'mnist'\n",
    "corruption = 'Noisy_Patrini_MNIST'\n",
    "corr_p = 0.7\n",
    "corr_n = None\n",
    "loss_type = 'Backward'\n",
    "for i in range(1):\n",
    "    base_dir = \"Datasets/weak_datasets\"\n",
    "    if corr_n is not None:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "    else:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "    f = open(folder_path + f'/Dataset_{i}.pkl','rb')\n",
    "    Data,Weak = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    if loss_type == 'Backward':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y,np.eye(Weak.c))\n",
    "    elif loss_type == 'Forward':\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'Forward_opt':\n",
    "        B = Weak.M @ torch.inverse(self.M.T @ torch.inverse(torch.diag(self.pest)) @ self.M ) @ self.M.T @ torch.inverse(torch.diag(self.pest))\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'EM':\n",
    "        loss_fn = losses.EMLoss(Weak.M)\n",
    "    elif loss_type == 'LBL':\n",
    "        loss_fn = losses.LBLoss()\n",
    "    elif loss_type == 'Backward_opt':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_opt_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'OSL':\n",
    "        loss_fn = losses.OSLCELoss()\n",
    "    \n",
    "    if loss_type == 'OSL':\n",
    "        Data.include_weak(Weak.w)\n",
    "    else:\n",
    "        Data.include_weak(Weak.z)\n",
    "\n",
    "    \n",
    "    trainloader,testloader = Data.get_dataloader(weak_labels = 'weak')\n",
    "\n",
    "    print(Data.num_features)\n",
    "\n",
    "\n",
    "    #trainloader,testloader = Data.get_dataloader()\n",
    "    mlp = MLP(Data.num_features, [], Weak.c, dropout_p=0, bn = False, activation='id')\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=1e-3,betas=(0.9, 0.999), eps=1e-8)\n",
    "    mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss_fn,corr_p=corr_p,num_epochs=10,sound=1,rep=i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bwd = FwdBwdLoss(pinv(M),I_c)\n",
    "# Fwd = FwdBwdLoss(I_d,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "Epoch 1/10: Train Loss: 2.4305, Train Acc: 0.8500, Test Acc: 0.8799, Train Detached Loss: 0.4478, Test Detached Loss: 0.4365, Learning Rate: 0.001000\n",
      "Epoch 2/10: Train Loss: 2.1110, Train Acc: 0.8773, Test Acc: 0.8975, Train Detached Loss: 0.5769, Test Detached Loss: 0.5284, Learning Rate: 0.001000\n",
      "Epoch 3/10: Train Loss: 2.0091, Train Acc: 0.8852, Test Acc: 0.8954, Train Detached Loss: 0.7091, Test Detached Loss: 0.6975, Learning Rate: 0.001000\n",
      "Epoch 4/10: Train Loss: 1.9082, Train Acc: 0.8916, Test Acc: 0.9044, Train Detached Loss: 0.7677, Test Detached Loss: 0.7466, Learning Rate: 0.001000\n",
      "Epoch 5/10: Train Loss: 1.8812, Train Acc: 0.8938, Test Acc: 0.8974, Train Detached Loss: 1.0000, Test Detached Loss: 0.9904, Learning Rate: 0.001000\n",
      "Epoch 6/10: Train Loss: 1.8379, Train Acc: 0.8967, Test Acc: 0.9083, Train Detached Loss: 0.9244, Test Detached Loss: 0.9052, Learning Rate: 0.001000\n",
      "Epoch 7/10: Train Loss: 1.8364, Train Acc: 0.8965, Test Acc: 0.9135, Train Detached Loss: 0.9985, Test Detached Loss: 0.9771, Learning Rate: 0.001000\n",
      "Epoch 8/10: Train Loss: 1.8091, Train Acc: 0.8988, Test Acc: 0.8919, Train Detached Loss: 1.1642, Test Detached Loss: 1.1901, Learning Rate: 0.001000\n",
      "Epoch 9/10: Train Loss: 1.8076, Train Acc: 0.8990, Test Acc: 0.9081, Train Detached Loss: 1.1457, Test Detached Loss: 1.1131, Learning Rate: 0.001000\n",
      "Epoch 10/10: Train Loss: 1.7423, Train Acc: 0.9024, Test Acc: 0.9024, Train Detached Loss: 1.2240, Test Detached Loss: 1.2287, Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "reps = 10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'mnist'\n",
    "corruption = 'Noisy_Patrini_MNIST'\n",
    "corr_p = 0.7\n",
    "corr_n = None\n",
    "loss_type = 'Backward_conv'\n",
    "for i in range(1):\n",
    "    base_dir = \"Datasets/weak_datasets\"\n",
    "    if corr_n is not None:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "    else:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "    f = open(folder_path + f'/Dataset_{i}.pkl','rb')\n",
    "    Data,Weak = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    if loss_type == 'Backward':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y,np.eye(Weak.c))\n",
    "    elif loss_type == 'Forward':\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'Forward_opt':\n",
    "        B = Weak.M @ torch.inverse(self.M.T @ torch.inverse(torch.diag(self.pest)) @ self.M ) @ self.M.T @ torch.inverse(torch.diag(self.pest))\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'EM':\n",
    "        loss_fn = losses.EMLoss(Weak.M)\n",
    "    elif loss_type == 'LBL':\n",
    "        loss_fn = losses.LBLoss()\n",
    "    elif loss_type == 'Backward_opt':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_opt_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'OSL':\n",
    "        loss_fn = losses.OSLCELoss()\n",
    "    \n",
    "    if loss_type == 'OSL':\n",
    "        Data.include_weak(Weak.w)\n",
    "    else:\n",
    "        Data.include_weak(Weak.z)\n",
    "\n",
    "    \n",
    "    trainloader,testloader = Data.get_dataloader(weak_labels = 'weak')\n",
    "\n",
    "    print(Data.num_features)\n",
    "\n",
    "\n",
    "    #trainloader,testloader = Data.get_dataloader()\n",
    "    mlp = MLP(Data.num_features, [], Weak.c, dropout_p=0, bn = False, activation='id')\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=1e-3,betas=(0.9, 0.999), eps=1e-8)\n",
    "    mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss_fn,corr_p=corr_p,num_epochs=10,sound=1,rep=i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as CSV at: Results\\image_pll_p0.5\\Forward.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_dir = f\"Results/{dataset}_{corruption}\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "if corr_n is not None:\n",
    "    file_name = f'{loss_type}_p_+{corr_p}p_-{corr_n}.csv'\n",
    "else:\n",
    "    file_name = f'{loss_type}_p_+{corr_p}p_-{corr_n}.csv'\n",
    "file_path = os.path.join(res_dir, file_name)\n",
    "results.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'DataFrame saved as CSV at: {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_detached_loss</th>\n",
       "      <th>test_detached_loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss_fn</th>\n",
       "      <th>repetition</th>\n",
       "      <th>initial_lr</th>\n",
       "      <th>actual_lr</th>\n",
       "      <th>corr_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.469290</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.066955</td>\n",
       "      <td>0.090892</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.441603</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.089786</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.413559</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>0.088971</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.388708</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.088314</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.371164</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.063951</td>\n",
       "      <td>0.087665</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>2.707886</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>0.062313</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>2.713032</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.034726</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>2.714207</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.034487</td>\n",
       "      <td>0.061459</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>2.714451</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.034288</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>2.725350</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.034119</td>\n",
       "      <td>0.060685</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
       "0       1    3.469290   0.196429  0.142857             0.066955   \n",
       "1       2    3.441603   0.285714  0.214286             0.065961   \n",
       "2       3    3.413559   0.285714  0.261905             0.065322   \n",
       "3       4    3.388708   0.285714  0.285714             0.064642   \n",
       "4       5    3.371164   0.285714  0.285714             0.063951   \n",
       "..    ...         ...        ...       ...                  ...   \n",
       "95     96    2.707886   0.583333  0.500000             0.035038   \n",
       "96     97    2.713032   0.607143  0.500000             0.034726   \n",
       "97     98    2.714207   0.595238  0.547619             0.034487   \n",
       "98     99    2.714451   0.601190  0.547619             0.034288   \n",
       "99    100    2.725350   0.601190  0.571429             0.034119   \n",
       "\n",
       "    test_detached_loss optimizer     loss_fn  repetition  initial_lr  \\\n",
       "0             0.090892      Adam  FwdBwdLoss           9       0.001   \n",
       "1             0.089786      Adam  FwdBwdLoss           9       0.001   \n",
       "2             0.088971      Adam  FwdBwdLoss           9       0.001   \n",
       "3             0.088314      Adam  FwdBwdLoss           9       0.001   \n",
       "4             0.087665      Adam  FwdBwdLoss           9       0.001   \n",
       "..                 ...       ...         ...         ...         ...   \n",
       "95            0.062313      Adam  FwdBwdLoss           9       0.001   \n",
       "96            0.061899      Adam  FwdBwdLoss           9       0.001   \n",
       "97            0.061459      Adam  FwdBwdLoss           9       0.001   \n",
       "98            0.061044      Adam  FwdBwdLoss           9       0.001   \n",
       "99            0.060685      Adam  FwdBwdLoss           9       0.001   \n",
       "\n",
       "    actual_lr  corr_p  \n",
       "0       0.001     0.5  \n",
       "1       0.001     0.5  \n",
       "2       0.001     0.5  \n",
       "3       0.001     0.5  \n",
       "4       0.001     0.5  \n",
       "..        ...     ...  \n",
       "95      0.001     0.5  \n",
       "96      0.001     0.5  \n",
       "97      0.001     0.5  \n",
       "98      0.001     0.5  \n",
       "99      0.001     0.5  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
