{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script reproduces what the main.py function does but divided into its parts so we can visualize the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Data_handling\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from utils.datasets_generation import generate_dataset\n",
    "import utils.losses as losses\n",
    "from utils.train_test_loop import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'image'\n",
    "corruption = 'pll'\n",
    "corr_p = 0.5\n",
    "corr_n = None\n",
    "\n",
    "#for i in range(reps):\n",
    "#        generate_dataset(dataset=dataset,corruption=corruption,corr_p=corr_p,repetitions=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danibacaicoa\\vscode_projects\\ForwardBackard_losses\\.venv\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1488, Test Acc: 0.1190, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1369, Test Acc: 0.1667, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1369, Test Acc: 0.1667, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1369, Test Acc: 0.1667, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1310, Test Acc: 0.1905, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1310, Test Acc: 0.1905, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1310, Test Acc: 0.1905, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1250, Test Acc: 0.2143, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1548, Test Acc: 0.0952, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1548, Test Acc: 0.0952, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1548, Test Acc: 0.0952, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 10/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 20/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n",
      "Epoch 30/30: Train Loss: nan, Train Acc: 0.1429, Test Acc: 0.1429, Train Detached Loss: nan, Test Detached Loss: nan, Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reps =10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'image'\n",
    "corruption = 'pll'\n",
    "corr_p = 0.5\n",
    "corr_n = None\n",
    "loss_type = 'Forward'\n",
    "for i in range(reps):\n",
    "    base_dir = \"Datasets/weak_datasets\"\n",
    "    if corr_n is not None:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "    else:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "    f = open(folder_path + f'/Dataset_{i}.pkl','rb')\n",
    "    Data,Weak = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    if loss_type == 'Backward':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y,np.eye(Weak.c))\n",
    "    elif loss_type == 'Forward':\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'EM':\n",
    "        loss_fn = losses.EMLoss(Weak.M)\n",
    "    elif loss_type == 'LBL':\n",
    "        loss_fn = losses.LBLoss()\n",
    "    elif loss_type == 'Backward_opt':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_opt_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'OSL':\n",
    "        loss_fn = losses.OSLCELoss()\n",
    "    \n",
    "    if loss_type == 'OSL':\n",
    "        Data.include_weak(Weak.w)\n",
    "    else:\n",
    "        Data.include_weak(Weak.z)\n",
    "\n",
    "    trainloader,testloader = Data.get_dataloader(weak_labels = 'weak')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #trainloader,testloader = Data.get_dataloader()\n",
    "    mlp = MLP(Data.num_features, [], Weak.c, dropout_p=0.3, bn = True, activation='relu')\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "    mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss_fn,num_epochs=30,sound=10,rep=i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bwd = FwdBwdLoss(pinv(M),I_c)\n",
    "# Fwd = FwdBwdLoss(I_d,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_detached_loss</th>\n",
       "      <th>test_detached_loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss_fn</th>\n",
       "      <th>repetition</th>\n",
       "      <th>initial_lr</th>\n",
       "      <th>actual_lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136905</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
       "0       1         NaN   0.136905  0.142857                  NaN   \n",
       "1       2         NaN   0.142857  0.142857                  NaN   \n",
       "2       3         NaN   0.142857  0.142857                  NaN   \n",
       "3       4         NaN   0.142857  0.142857                  NaN   \n",
       "4       5         NaN   0.142857  0.142857                  NaN   \n",
       "5       6         NaN   0.142857  0.142857                  NaN   \n",
       "6       7         NaN   0.142857  0.142857                  NaN   \n",
       "7       8         NaN   0.142857  0.142857                  NaN   \n",
       "8       9         NaN   0.142857  0.142857                  NaN   \n",
       "9      10         NaN   0.142857  0.142857                  NaN   \n",
       "10     11         NaN   0.142857  0.142857                  NaN   \n",
       "11     12         NaN   0.142857  0.142857                  NaN   \n",
       "12     13         NaN   0.142857  0.142857                  NaN   \n",
       "13     14         NaN   0.142857  0.142857                  NaN   \n",
       "14     15         NaN   0.142857  0.142857                  NaN   \n",
       "15     16         NaN   0.142857  0.142857                  NaN   \n",
       "16     17         NaN   0.142857  0.142857                  NaN   \n",
       "17     18         NaN   0.142857  0.142857                  NaN   \n",
       "18     19         NaN   0.142857  0.142857                  NaN   \n",
       "19     20         NaN   0.142857  0.142857                  NaN   \n",
       "20     21         NaN   0.142857  0.142857                  NaN   \n",
       "21     22         NaN   0.142857  0.142857                  NaN   \n",
       "22     23         NaN   0.142857  0.142857                  NaN   \n",
       "23     24         NaN   0.142857  0.142857                  NaN   \n",
       "24     25         NaN   0.142857  0.142857                  NaN   \n",
       "25     26         NaN   0.142857  0.142857                  NaN   \n",
       "26     27         NaN   0.142857  0.142857                  NaN   \n",
       "27     28         NaN   0.142857  0.142857                  NaN   \n",
       "28     29         NaN   0.142857  0.142857                  NaN   \n",
       "29     30         NaN   0.142857  0.142857                  NaN   \n",
       "\n",
       "    test_detached_loss optimizer     loss_fn  repetition  initial_lr  \\\n",
       "0                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "1                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "2                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "3                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "4                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "5                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "6                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "7                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "8                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "9                  NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "10                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "11                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "12                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "13                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "14                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "15                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "16                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "17                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "18                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "19                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "20                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "21                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "22                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "23                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "24                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "25                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "26                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "27                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "28                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "29                 NaN      Adam  FwdBwdLoss           9       0.001   \n",
       "\n",
       "    actual_lr  \n",
       "0       0.001  \n",
       "1       0.001  \n",
       "2       0.001  \n",
       "3       0.001  \n",
       "4       0.001  \n",
       "5       0.001  \n",
       "6       0.001  \n",
       "7       0.001  \n",
       "8       0.001  \n",
       "9       0.001  \n",
       "10      0.001  \n",
       "11      0.001  \n",
       "12      0.001  \n",
       "13      0.001  \n",
       "14      0.001  \n",
       "15      0.001  \n",
       "16      0.001  \n",
       "17      0.001  \n",
       "18      0.001  \n",
       "19      0.001  \n",
       "20      0.001  \n",
       "21      0.001  \n",
       "22      0.001  \n",
       "23      0.001  \n",
       "24      0.001  \n",
       "25      0.001  \n",
       "26      0.001  \n",
       "27      0.001  \n",
       "28      0.001  \n",
       "29      0.001  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
