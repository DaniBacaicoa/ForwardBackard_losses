{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import utils.losses as losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danib\\VSprojects\\ForwardBackard_losses\\utils\\losses.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.M = torch.tensor(M, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0004,  0.9539, -0.2323],\n",
      "        [-1.6134,  0.4867,  0.4534],\n",
      "        [ 0.9810, -0.1354,  0.3386],\n",
      "        [-1.0748,  0.9451, -0.1605],\n",
      "        [-0.0107,  0.4212, -0.2013],\n",
      "        [ 1.2680, -0.2674, -0.0358],\n",
      "        [-1.5818,  0.5448,  0.3425],\n",
      "        [-1.4912,  0.4040,  0.4957],\n",
      "        [-0.7380, -0.2345,  0.7433],\n",
      "        [-1.1654,  0.0842,  0.6868],\n",
      "        [ 1.1991, -0.1981,  0.3438],\n",
      "        [-0.0768,  0.2130,  0.2019],\n",
      "        [-0.5604,  1.0024, -0.3926],\n",
      "        [ 0.1147,  0.0500,  0.3928],\n",
      "        [-0.2036,  0.3479,  0.0650],\n",
      "        [-0.9503,  1.0625, -0.4224]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([1, 0, 2, 1, 1, 2, 0, 0, 0, 0, 2, 1, 2, 1, 1, 1])\n",
      "tensor([[-0.1322,  0.5988, -0.1344],\n",
      "        [ 0.9858, -0.3252,  0.5799],\n",
      "        [ 0.4653,  0.1160,  0.0486],\n",
      "        [-0.2590,  0.4482, -0.0502],\n",
      "        [-0.0491,  0.3205,  0.1261],\n",
      "        [-0.8440,  0.1667,  0.1022],\n",
      "        [ 1.0856, -0.1175, -0.1440],\n",
      "        [ 0.4092,  0.1519, -0.1061],\n",
      "        [-0.0557,  0.2286,  0.2640],\n",
      "        [-0.0778,  0.5248, -0.3064],\n",
      "        [-0.3232,  0.3432,  0.1470],\n",
      "        [ 0.1031, -0.0269,  0.4647],\n",
      "        [-0.0541, -0.6296,  0.7208],\n",
      "        [-1.3243,  1.2286, -0.5301],\n",
      "        [ 0.8429, -0.0152, -0.0293],\n",
      "        [ 0.5536, -0.0164,  0.5560]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([1, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2, 2])\n",
      "tensor([[-1.2162,  0.6277, -0.3569],\n",
      "        [-0.4641, -0.1473,  0.2003],\n",
      "        [-0.5651, -0.0759,  0.2809],\n",
      "        [ 0.7559, -0.1551,  0.3818],\n",
      "        [-0.0982, -0.4619,  0.4446],\n",
      "        [-0.0848,  0.5211,  0.0399],\n",
      "        [-0.5440,  0.7316, -0.1915],\n",
      "        [ 0.0088, -0.4212,  0.3471],\n",
      "        [-1.0159,  0.3754,  0.0564],\n",
      "        [-0.1043,  0.4532, -0.0082],\n",
      "        [-0.1401,  0.4269,  0.2187],\n",
      "        [-0.1082,  0.7382, -0.4998],\n",
      "        [-0.5465,  0.0989,  0.0944],\n",
      "        [ 0.3657,  0.1411,  0.2576],\n",
      "        [-0.0466,  0.2598,  0.2110],\n",
      "        [-1.1417,  0.6091, -0.3397]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([0, 0, 0, 2, 0, 2, 1, 0, 0, 2, 2, 2, 0, 2, 1, 0])\n",
      "tensor([[-0.3814,  0.0920, -0.3714],\n",
      "        [ 0.1581,  0.0626,  0.7911],\n",
      "        [-0.0762, -0.3431,  0.2171],\n",
      "        [ 0.6262, -0.0305,  0.1552],\n",
      "        [-0.5644,  0.8242, -0.4993],\n",
      "        [-0.3755,  0.1445, -0.4366],\n",
      "        [-0.1473,  0.7136, -0.3768],\n",
      "        [-0.2625,  0.1804,  0.4887],\n",
      "        [ 0.1150,  0.1333,  0.5508],\n",
      "        [-0.5173,  0.7763,  0.2227],\n",
      "        [ 0.0668, -0.0315,  1.0636],\n",
      "        [-1.2298,  0.9933,  0.0916],\n",
      "        [-1.0699,  1.1431, -0.6791],\n",
      "        [-0.6989,  0.6816, -0.0112],\n",
      "        [-0.6724,  0.8514, -0.2005],\n",
      "        [-0.6037,  1.0143, -0.3815]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([0, 2, 0, 2, 1, 0, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2])\n",
      "tensor([[-0.0551, -0.1200, -0.3010],\n",
      "        [ 0.8195, -0.5924,  1.1355],\n",
      "        [ 0.2592, -0.2800, -0.3561],\n",
      "        [ 0.4124, -0.1251,  0.6311],\n",
      "        [ 0.4366,  0.0708,  0.3562],\n",
      "        [-0.7713,  0.6457, -0.0423],\n",
      "        [ 0.8334, -1.2012,  0.4912],\n",
      "        [-0.1817,  0.3975,  0.4331],\n",
      "        [-0.1974, -0.0388, -0.3491],\n",
      "        [-0.7761,  0.7144,  0.3495],\n",
      "        [-0.5673,  0.5410, -0.8728],\n",
      "        [ 0.2191, -0.2294,  1.2406],\n",
      "        [-0.6068,  0.5731,  0.1192],\n",
      "        [-0.4539,  0.7539, -0.0845],\n",
      "        [-0.0077, -0.1037, -0.2963],\n",
      "        [ 1.0770, -0.9500,  1.7286]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([0, 2, 0, 2, 2, 1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 2])\n",
      "tensor([[ 0.5082, -0.4657, -0.3757],\n",
      "        [-0.0192, -0.3382,  1.2264],\n",
      "        [-0.1931,  0.3067,  0.2671],\n",
      "        [ 0.3650, -0.3925, -0.3258],\n",
      "        [ 0.7592, -0.7512, -0.1367],\n",
      "        [ 1.4095, -1.3961,  0.1706],\n",
      "        [ 0.0467,  0.0339,  0.6029],\n",
      "        [-0.2547,  0.0592,  0.8873],\n",
      "        [-1.0459,  1.0912, -1.2794],\n",
      "        [-0.1410, -0.1557,  1.1160],\n",
      "        [-0.8298,  0.9122, -0.2842],\n",
      "        [-0.7669,  1.2345, -0.8021],\n",
      "        [-0.3142, -0.0460,  1.7358],\n",
      "        [ 0.6601, -0.5083, -0.4833],\n",
      "        [-0.6211,  0.5393,  0.4301],\n",
      "        [-0.4447,  0.5156, -0.0136]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([0, 1, 2, 0, 0, 0, 2, 1, 0, 1, 1, 2, 2, 0, 2, 1])\n",
      "tensor([[-0.7436,  0.8373, -0.6750],\n",
      "        [-0.3377,  0.2798,  0.8023],\n",
      "        [-1.2238,  0.8912,  0.4067],\n",
      "        [-0.2738,  0.0579,  1.0761],\n",
      "        [-1.4208,  1.4715, -0.7845],\n",
      "        [-0.1557,  0.3170, -0.0297],\n",
      "        [-0.2125,  0.2545,  0.1878],\n",
      "        [ 0.7138, -0.5494, -0.5136],\n",
      "        [-0.2470,  0.4138,  0.2619]], device='cuda:0',\n",
      "       grad_fn=<ToCopyBackward0>)\n",
      "tensor([1, 2, 1, 2, 1, 1, 1, 0, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (105) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m _, y_pred_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(y_pred_test, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m (\u001b[43my_pred_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     82\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m (y_pred_test \u001b[38;5;241m==\u001b[39m y_test_tensor)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (105) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import utils.losses as losses\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_train_tensor = torch.eye(3)[y_train_tensor] \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define Logistic Regression model for Multiclass Classification\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # Softmax will be applied by the loss function\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = len(iris.target_names)  # Number of classes\n",
    "model = LogisticRegressionModel(input_dim=input_dim, output_dim=output_dim)\n",
    "loss_fn = losses.FwdLoss(torch.eye(3))\n",
    "#criterion = nn.CrossEntropyLoss()  # Cross-Entropy Loss for multiclass classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        y_batch = torch.max(y_batch,dim=1).indices\n",
    "        outputs = model(X_batch).to(device)\n",
    "        print(outputs)\n",
    "        print(y_batch)\n",
    "        loss,Mp,z = loss_fn(outputs, y_batch.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    y_pred_test = model(X_test_tensor)\n",
    "\n",
    "    # Convert logits to class labels\n",
    "    _, y_pred_train = torch.max(y_pred_train, 1)\n",
    "    _, y_pred_test = torch.max(y_pred_test, 1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = (y_pred_train == y_train_tensor).float().mean()\n",
    "    test_accuracy = (y_pred_test == y_test_tensor).float().mean()\n",
    "\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "M = np.array([[1,0,0,0,0,0,0,0,0,0],\n",
    "[0,1,0,0,0,0,0,0,0,0],\n",
    "[0,0,.3,0,0,0,0,.7,0,0],\n",
    "[0,0,0,.3,0,0,0,0,.7,0],\n",
    "[0,0,0,0,1,0,0,0,0,0],\n",
    "[0,0,0,0,0,.3,.7,0,0,0],\n",
    "[0,0,0,0,0,.7,.3,0,0,0],\n",
    "[0,.7,0,0,0,0,0,.3,0,0],\n",
    "[0,0,0,0,0,0,0,0,1,0],\n",
    "[0,0,0,0,0,0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0.7, 0. , 0. ],\n",
       "       [0. , 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.3, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.3, 0.7, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.7, 0.3, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.7, 0. , 0. , 0. , 0. , 0.3, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.7, 0. , 0. , 0. , 0. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n",
      "tensor([[0.1443, 0.1672, 0.0695, 0.1600, 0.0478, 0.2675, 0.2447, 0.6346, 0.2174],\n",
      "        [0.7012, 0.3100, 0.5758, 0.2229, 0.8619, 0.4291, 0.3903, 0.1794, 0.4210],\n",
      "        [0.1545, 0.5228, 0.3547, 0.6171, 0.0903, 0.3034, 0.3651, 0.1860, 0.3616]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Mp.shape)\n",
    "print(Mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 2, 1, 1, 1, 0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7012, 0.5228, 0.5758, 0.6171, 0.8619, 0.4291, 0.3903, 0.6346, 0.3616],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mp[z,range(Mp.size(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(Mp.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = y_batch.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mp = torch.tensor([[0.2607, 0.3270, 0.1814, 0.2682, 0.5227, 0.2219, 0.1594, 0.2619, 0.5241,\n",
    "         0.2541, 0.0906, 0.3955, 0.2038, 0.5019, 0.1821, 0.2850],\n",
    "        [0.3531, 0.3812, 0.4106, 0.4171, 0.3218, 0.4037, 0.3968, 0.3703, 0.3340,\n",
    "         0.4167, 0.4352, 0.3120, 0.3095, 0.2990, 0.3060, 0.3943],\n",
    "        [0.3861, 0.2918, 0.4081, 0.3147, 0.1554, 0.3744, 0.4438, 0.3677, 0.1418,\n",
    "         0.3292, 0.4742, 0.2925, 0.4866, 0.1991, 0.5119, 0.3207]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [9], [3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMp\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [9], [3]"
     ]
    }
   ],
   "source": [
    "Mp[y_batch,range(Mp.size(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
