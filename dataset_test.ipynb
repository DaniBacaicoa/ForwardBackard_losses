{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script reproduces what the main.py function does but divided into its parts so we can visualize the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Data_handling\n",
    "from src.weakener import Weakener\n",
    "from src.model import MLP\n",
    "from utils.datasets_generation import generate_dataset\n",
    "import utils.losses as losses\n",
    "from utils.train_test_loop import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'image'\n",
    "corruption = 'pll'\n",
    "corr_p = 0.5\n",
    "corr_n = None\n",
    "\n",
    "#for i in range(reps):\n",
    "#        generate_dataset(dataset=dataset,corruption=corruption,corr_p=corr_p,repetitions=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danibacaicoa\\vscode_projects\\ForwardBackard_losses\\.venv\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "c:\\Users\\danibacaicoa\\vscode_projects\\ForwardBackard_losses\\src\\model.py:18: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  nn.init.constant(layer.weight, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Train Loss: 3.4998, Train Acc: 0.2679, Test Acc: 0.2619, Train Detached Loss: 0.0635, Test Detached Loss: 0.0803, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.4357, Train Acc: 0.2798, Test Acc: 0.2619, Train Detached Loss: 0.0605, Test Detached Loss: 0.0758, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.3517, Train Acc: 0.3512, Test Acc: 0.3810, Train Detached Loss: 0.0572, Test Detached Loss: 0.0719, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.2387, Train Acc: 0.3810, Test Acc: 0.3810, Train Detached Loss: 0.0520, Test Detached Loss: 0.0651, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.1730, Train Acc: 0.4167, Test Acc: 0.4048, Train Detached Loss: 0.0479, Test Detached Loss: 0.0601, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 3.0865, Train Acc: 0.4345, Test Acc: 0.4048, Train Detached Loss: 0.0443, Test Detached Loss: 0.0564, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 3.0173, Train Acc: 0.5417, Test Acc: 0.5476, Train Detached Loss: 0.0413, Test Detached Loss: 0.0531, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.9776, Train Acc: 0.5714, Test Acc: 0.5714, Train Detached Loss: 0.0388, Test Detached Loss: 0.0503, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.9229, Train Acc: 0.6131, Test Acc: 0.5714, Train Detached Loss: 0.0369, Test Detached Loss: 0.0488, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.8783, Train Acc: 0.6488, Test Acc: 0.5714, Train Detached Loss: 0.0350, Test Detached Loss: 0.0462, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.2480, Train Acc: 0.2917, Test Acc: 0.1667, Train Detached Loss: 0.0619, Test Detached Loss: 0.0847, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.1843, Train Acc: 0.2976, Test Acc: 0.1667, Train Detached Loss: 0.0590, Test Detached Loss: 0.0807, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.1364, Train Acc: 0.3095, Test Acc: 0.1905, Train Detached Loss: 0.0568, Test Detached Loss: 0.0777, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.0408, Train Acc: 0.4881, Test Acc: 0.5476, Train Detached Loss: 0.0529, Test Detached Loss: 0.0715, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 2.9648, Train Acc: 0.5179, Test Acc: 0.5476, Train Detached Loss: 0.0488, Test Detached Loss: 0.0650, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 2.8842, Train Acc: 0.5833, Test Acc: 0.5714, Train Detached Loss: 0.0452, Test Detached Loss: 0.0594, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.8242, Train Acc: 0.6131, Test Acc: 0.5714, Train Detached Loss: 0.0421, Test Detached Loss: 0.0546, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.7336, Train Acc: 0.6607, Test Acc: 0.5714, Train Detached Loss: 0.0392, Test Detached Loss: 0.0503, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.6751, Train Acc: 0.6607, Test Acc: 0.5714, Train Detached Loss: 0.0369, Test Detached Loss: 0.0474, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.6612, Train Acc: 0.6488, Test Acc: 0.5952, Train Detached Loss: 0.0349, Test Detached Loss: 0.0444, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.3660, Train Acc: 0.2917, Test Acc: 0.2143, Train Detached Loss: 0.0615, Test Detached Loss: 0.1012, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.3040, Train Acc: 0.2798, Test Acc: 0.2143, Train Detached Loss: 0.0592, Test Detached Loss: 0.0968, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.2739, Train Acc: 0.2917, Test Acc: 0.2143, Train Detached Loss: 0.0575, Test Detached Loss: 0.0954, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.2141, Train Acc: 0.3274, Test Acc: 0.2619, Train Detached Loss: 0.0547, Test Detached Loss: 0.0962, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.1230, Train Acc: 0.3631, Test Acc: 0.2857, Train Detached Loss: 0.0507, Test Detached Loss: 0.1081, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 3.0479, Train Acc: 0.3810, Test Acc: 0.2857, Train Detached Loss: 0.0475, Test Detached Loss: 0.1244, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.9973, Train Acc: 0.4464, Test Acc: 0.3095, Train Detached Loss: 0.0442, Test Detached Loss: 0.1384, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.9172, Train Acc: 0.4940, Test Acc: 0.3810, Train Detached Loss: 0.0411, Test Detached Loss: 0.1520, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.8513, Train Acc: 0.5655, Test Acc: 0.4762, Train Detached Loss: 0.0378, Test Detached Loss: 0.1650, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.7898, Train Acc: 0.7024, Test Acc: 0.6429, Train Detached Loss: 0.0345, Test Detached Loss: 0.1620, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.4341, Train Acc: 0.2857, Test Acc: 0.2857, Train Detached Loss: 0.0622, Test Detached Loss: 0.0854, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.3742, Train Acc: 0.2857, Test Acc: 0.2857, Train Detached Loss: 0.0598, Test Detached Loss: 0.0826, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.3367, Train Acc: 0.2857, Test Acc: 0.2857, Train Detached Loss: 0.0577, Test Detached Loss: 0.0803, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.2553, Train Acc: 0.4048, Test Acc: 0.4762, Train Detached Loss: 0.0547, Test Detached Loss: 0.0765, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.1623, Train Acc: 0.4405, Test Acc: 0.4762, Train Detached Loss: 0.0506, Test Detached Loss: 0.0710, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 3.1082, Train Acc: 0.4345, Test Acc: 0.4762, Train Detached Loss: 0.0475, Test Detached Loss: 0.0675, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 3.0403, Train Acc: 0.5417, Test Acc: 0.5952, Train Detached Loss: 0.0442, Test Detached Loss: 0.0641, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.9839, Train Acc: 0.6012, Test Acc: 0.6190, Train Detached Loss: 0.0409, Test Detached Loss: 0.0604, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.9295, Train Acc: 0.6190, Test Acc: 0.6190, Train Detached Loss: 0.0380, Test Detached Loss: 0.0574, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.8656, Train Acc: 0.6250, Test Acc: 0.5952, Train Detached Loss: 0.0354, Test Detached Loss: 0.0550, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.1982, Train Acc: 0.3036, Test Acc: 0.3095, Train Detached Loss: 0.0616, Test Detached Loss: 0.0784, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.1284, Train Acc: 0.2917, Test Acc: 0.3095, Train Detached Loss: 0.0587, Test Detached Loss: 0.0749, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.0629, Train Acc: 0.3095, Test Acc: 0.3095, Train Detached Loss: 0.0565, Test Detached Loss: 0.0729, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 2.9970, Train Acc: 0.3869, Test Acc: 0.4286, Train Detached Loss: 0.0538, Test Detached Loss: 0.0703, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 2.8986, Train Acc: 0.4048, Test Acc: 0.4048, Train Detached Loss: 0.0496, Test Detached Loss: 0.0656, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 2.8191, Train Acc: 0.5476, Test Acc: 0.5238, Train Detached Loss: 0.0458, Test Detached Loss: 0.0610, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.7443, Train Acc: 0.5893, Test Acc: 0.5714, Train Detached Loss: 0.0422, Test Detached Loss: 0.0564, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.6603, Train Acc: 0.6250, Test Acc: 0.5952, Train Detached Loss: 0.0392, Test Detached Loss: 0.0528, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.6316, Train Acc: 0.6190, Test Acc: 0.6429, Train Detached Loss: 0.0367, Test Detached Loss: 0.0494, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.5776, Train Acc: 0.6131, Test Acc: 0.6429, Train Detached Loss: 0.0343, Test Detached Loss: 0.0462, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.3269, Train Acc: 0.1726, Test Acc: 0.1429, Train Detached Loss: 0.0633, Test Detached Loss: 0.0814, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.2786, Train Acc: 0.1726, Test Acc: 0.1429, Train Detached Loss: 0.0608, Test Detached Loss: 0.0784, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.2373, Train Acc: 0.2202, Test Acc: 0.1667, Train Detached Loss: 0.0587, Test Detached Loss: 0.0759, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.1638, Train Acc: 0.2738, Test Acc: 0.3095, Train Detached Loss: 0.0554, Test Detached Loss: 0.0712, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.0922, Train Acc: 0.3929, Test Acc: 0.4286, Train Detached Loss: 0.0515, Test Detached Loss: 0.0659, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 3.0014, Train Acc: 0.3333, Test Acc: 0.4524, Train Detached Loss: 0.0479, Test Detached Loss: 0.0622, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.9304, Train Acc: 0.4643, Test Acc: 0.4762, Train Detached Loss: 0.0443, Test Detached Loss: 0.0594, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.8326, Train Acc: 0.4940, Test Acc: 0.5000, Train Detached Loss: 0.0406, Test Detached Loss: 0.0559, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.7739, Train Acc: 0.5833, Test Acc: 0.5714, Train Detached Loss: 0.0373, Test Detached Loss: 0.0521, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.7374, Train Acc: 0.5893, Test Acc: 0.6905, Train Detached Loss: 0.0341, Test Detached Loss: 0.0471, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.2510, Train Acc: 0.2857, Test Acc: 0.2619, Train Detached Loss: 0.0626, Test Detached Loss: 0.0799, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.1859, Train Acc: 0.2857, Test Acc: 0.2619, Train Detached Loss: 0.0597, Test Detached Loss: 0.0756, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.1428, Train Acc: 0.2917, Test Acc: 0.2619, Train Detached Loss: 0.0576, Test Detached Loss: 0.0727, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.0771, Train Acc: 0.3452, Test Acc: 0.3095, Train Detached Loss: 0.0549, Test Detached Loss: 0.0686, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 2.9762, Train Acc: 0.3869, Test Acc: 0.4048, Train Detached Loss: 0.0505, Test Detached Loss: 0.0620, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 2.9221, Train Acc: 0.4345, Test Acc: 0.4524, Train Detached Loss: 0.0471, Test Detached Loss: 0.0576, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.8591, Train Acc: 0.4643, Test Acc: 0.4762, Train Detached Loss: 0.0437, Test Detached Loss: 0.0539, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.7380, Train Acc: 0.5060, Test Acc: 0.5238, Train Detached Loss: 0.0402, Test Detached Loss: 0.0504, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.6673, Train Acc: 0.6429, Test Acc: 0.6190, Train Detached Loss: 0.0367, Test Detached Loss: 0.0465, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.6133, Train Acc: 0.6905, Test Acc: 0.7143, Train Detached Loss: 0.0328, Test Detached Loss: 0.0403, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.4389, Train Acc: 0.2798, Test Acc: 0.2143, Train Detached Loss: 0.0611, Test Detached Loss: 0.0867, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.3707, Train Acc: 0.2917, Test Acc: 0.2143, Train Detached Loss: 0.0587, Test Detached Loss: 0.0850, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.3076, Train Acc: 0.3095, Test Acc: 0.2143, Train Detached Loss: 0.0558, Test Detached Loss: 0.0845, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.1872, Train Acc: 0.3929, Test Acc: 0.3333, Train Detached Loss: 0.0509, Test Detached Loss: 0.0823, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.1213, Train Acc: 0.4167, Test Acc: 0.3333, Train Detached Loss: 0.0471, Test Detached Loss: 0.0753, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 3.0323, Train Acc: 0.4524, Test Acc: 0.3810, Train Detached Loss: 0.0436, Test Detached Loss: 0.0693, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.9633, Train Acc: 0.5000, Test Acc: 0.4524, Train Detached Loss: 0.0401, Test Detached Loss: 0.0661, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.9242, Train Acc: 0.5714, Test Acc: 0.4286, Train Detached Loss: 0.0370, Test Detached Loss: 0.0645, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.8300, Train Acc: 0.6250, Test Acc: 0.4762, Train Detached Loss: 0.0341, Test Detached Loss: 0.0615, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.7863, Train Acc: 0.6429, Test Acc: 0.5000, Train Detached Loss: 0.0312, Test Detached Loss: 0.0576, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.2760, Train Acc: 0.2976, Test Acc: 0.2381, Train Detached Loss: 0.0601, Test Detached Loss: 0.0824, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.2198, Train Acc: 0.2976, Test Acc: 0.2381, Train Detached Loss: 0.0573, Test Detached Loss: 0.0783, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.1673, Train Acc: 0.3095, Test Acc: 0.2381, Train Detached Loss: 0.0554, Test Detached Loss: 0.0752, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.0997, Train Acc: 0.4107, Test Acc: 0.4048, Train Detached Loss: 0.0525, Test Detached Loss: 0.0685, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.0013, Train Acc: 0.4167, Test Acc: 0.4048, Train Detached Loss: 0.0485, Test Detached Loss: 0.0588, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 2.9478, Train Acc: 0.4643, Test Acc: 0.4762, Train Detached Loss: 0.0455, Test Detached Loss: 0.0524, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.8864, Train Acc: 0.5119, Test Acc: 0.5476, Train Detached Loss: 0.0426, Test Detached Loss: 0.0472, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.8330, Train Acc: 0.5774, Test Acc: 0.6905, Train Detached Loss: 0.0402, Test Detached Loss: 0.0423, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.7923, Train Acc: 0.5774, Test Acc: 0.7143, Train Detached Loss: 0.0377, Test Detached Loss: 0.0378, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.7115, Train Acc: 0.6190, Test Acc: 0.7143, Train Detached Loss: 0.0351, Test Detached Loss: 0.0339, Learning Rate: 0.001000\n",
      "Epoch 10/100: Train Loss: 3.2986, Train Acc: 0.2857, Test Acc: 0.2857, Train Detached Loss: 0.0611, Test Detached Loss: 0.0849, Learning Rate: 0.001000\n",
      "Epoch 20/100: Train Loss: 3.2471, Train Acc: 0.2857, Test Acc: 0.2857, Train Detached Loss: 0.0582, Test Detached Loss: 0.0826, Learning Rate: 0.001000\n",
      "Epoch 30/100: Train Loss: 3.1833, Train Acc: 0.2679, Test Acc: 0.1905, Train Detached Loss: 0.0560, Test Detached Loss: 0.0808, Learning Rate: 0.001000\n",
      "Epoch 40/100: Train Loss: 3.0881, Train Acc: 0.3750, Test Acc: 0.2381, Train Detached Loss: 0.0522, Test Detached Loss: 0.0781, Learning Rate: 0.001000\n",
      "Epoch 50/100: Train Loss: 3.0234, Train Acc: 0.3810, Test Acc: 0.2381, Train Detached Loss: 0.0480, Test Detached Loss: 0.0758, Learning Rate: 0.001000\n",
      "Epoch 60/100: Train Loss: 2.9431, Train Acc: 0.4048, Test Acc: 0.2381, Train Detached Loss: 0.0445, Test Detached Loss: 0.0731, Learning Rate: 0.001000\n",
      "Epoch 70/100: Train Loss: 2.8682, Train Acc: 0.4643, Test Acc: 0.2857, Train Detached Loss: 0.0412, Test Detached Loss: 0.0698, Learning Rate: 0.001000\n",
      "Epoch 80/100: Train Loss: 2.8094, Train Acc: 0.4821, Test Acc: 0.3095, Train Detached Loss: 0.0386, Test Detached Loss: 0.0673, Learning Rate: 0.001000\n",
      "Epoch 90/100: Train Loss: 2.7400, Train Acc: 0.5714, Test Acc: 0.5000, Train Detached Loss: 0.0364, Test Detached Loss: 0.0644, Learning Rate: 0.001000\n",
      "Epoch 100/100: Train Loss: 2.7253, Train Acc: 0.6012, Test Acc: 0.5714, Train Detached Loss: 0.0341, Test Detached Loss: 0.0607, Learning Rate: 0.001000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reps =10\n",
    "dataset_base_path = 'Datasets/weak_datasets'\n",
    "dataset = 'image'\n",
    "corruption = 'pll'\n",
    "corr_p = 0.5\n",
    "corr_n = None\n",
    "loss_type = 'Forward'\n",
    "for i in range(reps):\n",
    "    base_dir = \"Datasets/weak_datasets\"\n",
    "    if corr_n is not None:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "    else:\n",
    "        folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "    f = open(folder_path + f'/Dataset_{i}.pkl','rb')\n",
    "    Data,Weak = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    if loss_type == 'Backward':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y,np.eye(Weak.c))\n",
    "    elif loss_type == 'Forward':\n",
    "        loss_fn = losses.FwdBwdLoss(np.eye(Weak.d),Weak.M)\n",
    "    elif loss_type == 'EM':\n",
    "        loss_fn = losses.EMLoss(Weak.M)\n",
    "    elif loss_type == 'LBL':\n",
    "        loss_fn = losses.LBLoss()\n",
    "    elif loss_type == 'Backward_opt':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'Backward_opt_conv':\n",
    "        loss_fn = losses.FwdBwdLoss(Weak.Y_opt_conv,np.eye(Weak.c))\n",
    "    elif loss_type == 'OSL':\n",
    "        loss_fn = losses.OSLCELoss()\n",
    "    \n",
    "    if loss_type == 'OSL':\n",
    "        Data.include_weak(Weak.w)\n",
    "    else:\n",
    "        Data.include_weak(Weak.z)\n",
    "\n",
    "    trainloader,testloader = Data.get_dataloader(weak_labels = 'weak')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #trainloader,testloader = Data.get_dataloader()\n",
    "    mlp = MLP(Data.num_features, [Data.num_features], Weak.c, dropout_p=0.3, bn = True, activation='relu')\n",
    "    optim = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "    mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss_fn,corr_p=corr_p,num_epochs=100,sound=10,rep=i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bwd = FwdBwdLoss(pinv(M),I_c)\n",
    "# Fwd = FwdBwdLoss(I_d,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as CSV at: Results\\image_pll_p0.5\\Forward.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = \"Results\"\n",
    "if corr_n is not None:\n",
    "    folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p_+{corr_p}p_-{corr_n}')\n",
    "else:\n",
    "    folder_path = os.path.join(base_dir, f'{dataset}_{corruption}_p{corr_p}')\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Specify the file name\n",
    "file_name = f'{loss_type}.csv'\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "results.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'DataFrame saved as CSV at: {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_detached_loss</th>\n",
       "      <th>test_detached_loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss_fn</th>\n",
       "      <th>repetition</th>\n",
       "      <th>initial_lr</th>\n",
       "      <th>actual_lr</th>\n",
       "      <th>corr_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.469290</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.066955</td>\n",
       "      <td>0.090892</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.441603</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.089786</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.413559</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>0.088971</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.388708</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.088314</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.371164</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.063951</td>\n",
       "      <td>0.087665</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>2.707886</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.035038</td>\n",
       "      <td>0.062313</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>2.713032</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.034726</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>2.714207</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.034487</td>\n",
       "      <td>0.061459</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>2.714451</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.034288</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>2.725350</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.034119</td>\n",
       "      <td>0.060685</td>\n",
       "      <td>Adam</td>\n",
       "      <td>FwdBwdLoss</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  train_acc  test_acc  train_detached_loss  \\\n",
       "0       1    3.469290   0.196429  0.142857             0.066955   \n",
       "1       2    3.441603   0.285714  0.214286             0.065961   \n",
       "2       3    3.413559   0.285714  0.261905             0.065322   \n",
       "3       4    3.388708   0.285714  0.285714             0.064642   \n",
       "4       5    3.371164   0.285714  0.285714             0.063951   \n",
       "..    ...         ...        ...       ...                  ...   \n",
       "95     96    2.707886   0.583333  0.500000             0.035038   \n",
       "96     97    2.713032   0.607143  0.500000             0.034726   \n",
       "97     98    2.714207   0.595238  0.547619             0.034487   \n",
       "98     99    2.714451   0.601190  0.547619             0.034288   \n",
       "99    100    2.725350   0.601190  0.571429             0.034119   \n",
       "\n",
       "    test_detached_loss optimizer     loss_fn  repetition  initial_lr  \\\n",
       "0             0.090892      Adam  FwdBwdLoss           9       0.001   \n",
       "1             0.089786      Adam  FwdBwdLoss           9       0.001   \n",
       "2             0.088971      Adam  FwdBwdLoss           9       0.001   \n",
       "3             0.088314      Adam  FwdBwdLoss           9       0.001   \n",
       "4             0.087665      Adam  FwdBwdLoss           9       0.001   \n",
       "..                 ...       ...         ...         ...         ...   \n",
       "95            0.062313      Adam  FwdBwdLoss           9       0.001   \n",
       "96            0.061899      Adam  FwdBwdLoss           9       0.001   \n",
       "97            0.061459      Adam  FwdBwdLoss           9       0.001   \n",
       "98            0.061044      Adam  FwdBwdLoss           9       0.001   \n",
       "99            0.060685      Adam  FwdBwdLoss           9       0.001   \n",
       "\n",
       "    actual_lr  corr_p  \n",
       "0       0.001     0.5  \n",
       "1       0.001     0.5  \n",
       "2       0.001     0.5  \n",
       "3       0.001     0.5  \n",
       "4       0.001     0.5  \n",
       "..        ...     ...  \n",
       "95      0.001     0.5  \n",
       "96      0.001     0.5  \n",
       "97      0.001     0.5  \n",
       "98      0.001     0.5  \n",
       "99      0.001     0.5  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
